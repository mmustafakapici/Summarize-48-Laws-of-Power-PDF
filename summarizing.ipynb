{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"choose a pdf file to read\")\n",
    "\n",
    "\n",
    "#file = input()\n",
    "\n",
    "file =\"48 Laws of Power, The - Robert Greene.pdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save extracted text to just one file seperated page by page\n",
    "\n",
    "def save_pdf(file):\n",
    "    pdf = PdfReader(file)\n",
    "    with open(\"extracted_text.txt\", \"w\") as f:\n",
    "        for i in range(21,530):\n",
    "            f.write(pdf.pages[i].extract_text())\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "save_pdf(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path where the files will be saved\n",
    "path = \"Laws/\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Read the content of the uploaded file\n",
    "with open(\"extracted_text.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Initialize variables to store the current law section and the law number\n",
    "law_number = 1\n",
    "current_law = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process each line to separate the sections\n",
    "for line in lines:\n",
    "    if line.startswith(\"LAW\"):\n",
    "        if current_law:\n",
    "            # Save the current law section to a file\n",
    "            with open(f\"{path}law_{law_number}.txt\", \"w\") as law_file:\n",
    "                law_file.writelines(current_law)\n",
    "            # Reset the current law section and increment the law number\n",
    "            current_law = []\n",
    "            law_number += 1\n",
    "    # Append the line to the current law section\n",
    "    current_law.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the last law section to a file if it exists\n",
    "if current_law:\n",
    "    with open(f\"{path}law_{law_number}.txt\", \"w\") as law_file:\n",
    "        law_file.writelines(current_law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# şimdi her bir yasayı özetleyelim ve bir dosyaya yazalım\n",
    "\n",
    "import re\n",
    "\n",
    "# Define the path where the files will be saved\n",
    "\n",
    "path = \"Summaries/\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of each law use LLM use Bert\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each law section\n",
    "\n",
    "for i in range(1, 49):\n",
    "    # Read the content of the law section\n",
    "    with open(f\"Laws/law_{i}.txt\", \"r\") as file:\n",
    "        content = file.read()\n",
    "    # Tokenize the content\n",
    "    inputs = tokenizer(content, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Perform the classification\n",
    "    outputs = model(**inputs)\n",
    "    # Get the predicted label\n",
    "    predicted_label = outputs.logits.argmax().item()\n",
    "    # Save the summary to a file\n",
    "    with open(f\"{path}summary_{i}.txt\", \"w\") as file:\n",
    "        file.write(f\"Law {i}\\n\")\n",
    "        file.write(f\"Predicted label: {predicted_label}\\n\")\n",
    "        file.write(content)\n",
    "        file.write(\"\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the summaries to search for specific laws based on the predicted label\n",
    "\n",
    "# Define the search query\n",
    "query = \"contract\"\n",
    "\n",
    "# Process each summary file\n",
    "\n",
    "for i in range(1, 49):\n",
    "\n",
    "    # Read the content of the summary\n",
    "    with open(f\"Summaries/summary_{i}.txt\", \"r\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Check if the query is present in the content\n",
    "    if query in content:\n",
    "        print(f\"Law {i} contains the query '{query}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate our laws to turkish use T5\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each law section\n",
    "\n",
    "for i in range(1, 49):\n",
    "    # Read the content of the law section\n",
    "    with open(f\"Laws/law_{i}.txt\", \"r\") as file:\n",
    "        content = file.read()\n",
    "    # Translate the content\n",
    "    inputs = tokenizer(f\"translate English to Turkish: {content}\", return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs)\n",
    "    translated_content = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Save the translation to a file\n",
    "    with open(f\"Translations/translation_{i}.txt\", \"w\") as file:\n",
    "        file.write(f\"Law {i}\\n\")\n",
    "        file.write(translated_content)\n",
    "        file.write(\"\\n\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the translated laws and search for a specific term\n",
    "\n",
    "# Define the search query\n",
    "query = \"sözleşme\"\n",
    "\n",
    "# Process each translation file\n",
    "for i in range(1, 49):\n",
    "    # Read the content of the translation\n",
    "    with open(f\"Translations/translation_{i}.txt\", \"r\") as file:\n",
    "        content = file.read()\n",
    "    # Check if the query is present in the content\n",
    "    if query in content:\n",
    "        print(f\"Law {i} contains the query '{query}'\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
